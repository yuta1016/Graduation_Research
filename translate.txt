・リズム
2.4. 周期性ヒストグラム (PH)
周期性ヒストグラムは、もともとビートトラッキングの文脈で提示されました [14]。同様のアプローチがジャンル分類にも開発されました [5]。我々が用いる類似度指標とこれら2つのアプローチの違いに関する詳細は、[7] に記載されています。
その考え方は、周波数に関わらず、周期的に繰り返されるビート（のみ）を記述することです。特徴は、Sone/Bark表現をさらに処理することで抽出されます。まず、打楽器音を強調するために、各Barkバンドに半波整流差分フィルタを適用します。次に、信号を12秒のセグメントに分割し、個別にさらに処理します。各シーケンスは、Hannウィンドウを用いて重み付けされた後、40bpmから240bpmの範囲で5bpmの分解能を持つコムフィルタバンクを各Barkバンドに適用します。次に、コムフィルタから得られた振幅に共鳴モデルを適用します。特定の周期におけるピークを強調するために、全帯域にわたって各周期の振幅を合計する前に、全波整流差分フィルタが使用されます。
12秒間の表現は、異なる周波数（bpm）を表す40の等間隔の列と、強度レベルを表す50の行を持つ2次元ヒストグラムを使用して要約されます。ヒストグラムは、各周期について、特定の値以上のレベルに達した回数をカウントします。2つのPH間の距離は、2つのSH間の距離と同じ方法で計算されます。
図2dは、最初のピークが60bpm付近、
80bpm付近の小さなピーク、120bpm付近の高いピーク、そして160bpm付近の非常に小さなピークが続くPHを示しています。

sss

FP [4] とPHの主な違いの一つは、
FPはBarkバンドの周期性を検出するために、計算コストの高いくし形フィルタではなく、単純なFFTを使用していることです。さらに、
PHは約120bpmで最大値を示す共鳴モデルを使用するのに対し、FPは4Hz（240bpm）でピークを示す変動モデルを使用しています。しかし、最大の違いは、
FPはスペクトルに関する情報を含むのに対し、PHはこの情報を無視していることです。
図2eは、2Hz（120bpm）付近の最も低い3つのBark帯域にピークがあり、4Hzでは全帯域にわたってより弱いピークがあるFPを示しています。PHとは異なり、2Hz未満のピークは見られません。

sss

楽曲の特定の瞬間におけるリズムの特徴を定量化するために、変動パターン（FP）[31]を用います。音声信号には、1秒ごとに移動する2.97秒長のハミング窓が適用されます。窓処理された音声セグメントはさらに256フレーム（44.1kHzサンプリングで512サンプルを含む）に分割されます。各フレームに対して高速フーリエ変換（FFT）を適用し、各周波数帯域の周期性ヒストグラム[31]を測定することで、フレームのFPを取得します。 256個のFPの平均値がセグメントのリズム成分として定義されます。


-------------------------------------------
・timber
楽曲の音色成分は、主に周波数成分によって決定されます。そのため、音声信号のスペクトル的側面の知覚的表現に広く用いられるメル周波数ケプストラム係数（MFCC）[32]を用います。リズムの場合と同様に、与えられた音声信号に2.97秒のハミング窓を適用し、窓処理されたセグメントを256フレームに分割します。各フレームには、36個のメル周波数間隔のビンを使用します。最後に、256フレームのMFCCを平均化します。


--------------------------------------------
・arousal
覚醒度：心理音響理論に基づく別の種類の複雑性特性、Arousal（覚醒度）を使用します。音楽の音量は感情の覚醒度と相関していることが知られています[33]。また、感情は音楽の嗜好を決定する重要な要因です[34]。したがって、音楽の嗜好（ひいては音楽の人気）は覚醒度によって影響を受けると推測できます。
2.97秒のハミング窓を用いて1秒ずつ移動させ、得られたセグメントの短時間振幅（すなわち、信号絶対値の合計）を計算します。次に、短時間振幅の平均値（ArousalMean）と標準偏差（ArousalStd）を時間経過にわたって計算し、音量の平均値と変動をそれぞれ測定します。


----------------------------------------
・sc
各成分についてSCを計算し、時間経過に伴う複雑性特徴を得る。観測ウィンドウの長さがwj = 2j−1であるi番目のセグメントのSCは、次式で与えられる。
SCij = JSD(si−wj:i−1,si:i+wj−1)
(2)
ここで、JSD(x,y)はxとyの間のJenson-Shannonダイバージェンス、sa:bはa番目からb番目のセグメントのsの値の合計である。観測ウィンドウは複雑性を測定する時間範囲を決定する。クロマについてはj =3,4,...,8、リズムと音色についてはj =1,2,...,6とし、これらは1秒から32秒のウィンドウ長に対応する。最後に、時間経過に伴うSCの平均値をとり、各特徴と各ウィンドウサイズについて単一の値を得る。その結果、クロマ、リズム、音色の各要素について、6つの観測ウィンドウ長に対する6つの複雑度特徴量（クロマ1～クロマ6、リズム1～リズム6、音色1～音色6）が得られます。


------------------------------------------------
・MFCC
compから
[21]では、ヒット曲予測のための楽曲のスペクトル特性を測定するためにMFCCに基づく特徴量が使用された。本研究でも[21]の特徴抽出手順を実装することでこれを採用する。
各オーディオ信号は、隣接するセグメント間のオーバーラップが0.015秒である長さ0.025秒のセグメントに分割され、各セグメントから20個のMFCCが抽出される。学習データセット内の楽曲の抽出されたMFCCに対して、ノイズを低減しコンパクトな特徴量を得るために、k-meansクラスタリングが実行され、32個のクラスター重心が得られる。これらの重心は、学習データで見つかった最も一般的な音響特性を表す。テスト用の楽曲が与えられると、そのMFCCベクトルの最小距離重心が検出され、32個のクラスターの正規化された周波数が楽曲の特徴量として得られる。

mfccの方から
まず、1700曲の実験データベースにある各曲を音響および歌詞ベースの表現に変換します。
前述のように、各曲を音響表現に変換する最初のステップは、一般的なオーディオセットからN個の最も顕著なクラスターを学習することです。具体的には、まずトレーニングセット内の各曲を、10msごとにサンプリングされた25msの重複ウィンドウから計算された20次元のMFCCベクトルに変換します。各ベクトルの0番目（DC）成分を破棄し、次にK平均法クラスタリングを実行してN個の最も顕著な音を学習します。計算上の理由から、
音響データがある18,500曲すべてを使用してこれらのクラスターを学習するわけではありません。
代わりに、このデータベースから約200曲をサンプリングしてK平均法モデルを学習します。
次に、実験データベース内の各曲を次のようにN次元ベクトルに変換します。
前と同様に、各曲を一連のMFCCベクトルに変換します。各ベクトルについて、N個のクラスターそれぞれに対してスコアリングを行い、スコアが最も高いクラスターのカウンターを1つ増やします。正規化されたカウントのセットが、その曲のN次元表現を形成します。
音響の場合と同様に、歌詞をN次元表現に変換する最初のステップは、テキストコーパスからトピックのセットを学習することです。私たちは歌詞のセットをコーパスとして使用し、ストップワードを削除した後、[7]で説明されているアルゴリズムに従って、約91,000語の辞書を使用してトピックをトレーニングしました。次に、これらのモデルに対して各曲の歌詞にスコアリングを行い、上記の音響の場合と同様に、正規化されたカウントのベクトルを生成します。